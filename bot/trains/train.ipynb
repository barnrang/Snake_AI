{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "sys.path.append('game/')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from base_network import BaseNetwork\n",
    "from keras.models import load_model\n",
    "from snake_env import SnakeEnvironment\n",
    "\n",
    "EP = 100000\n",
    "\n",
    "class Config:\n",
    "    height = 20\n",
    "    width = 30\n",
    "    action_num = 5\n",
    "    lr = 3e-5\n",
    "    eps = 1.\n",
    "    eps_min = 0.1\n",
    "    eps_decay = 0.999\n",
    "    gamma = 0.95\n",
    "\n",
    "class DQAgent():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.memory = deque(maxlen=20000)\n",
    "\n",
    "        # model\n",
    "        self.q = BaseNetwork(config.height, config.width, config.action_num, config) \n",
    "        # target model\n",
    "        self.qt = BaseNetwork(config.height, config.width, config.action_num, config)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.uniform(0, 1) < self.config.eps:\n",
    "            return random.randrange(self.config.action_num)\n",
    "        else:\n",
    "            print('predict')\n",
    "            return np.argmax(self.q.model.predict(self.rescale_color(state))[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        print('do_replay')\n",
    "        if batch_size > len(self.memory):\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #print(state, action, reward, next_state, done)\n",
    "            inpu = self.rescale_color(state)\n",
    "            target = self.q.model.predict(inpu)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                inp = self.rescale_color(next_state)\n",
    "                a = self.q.model.predict(inp)[0]\n",
    "                t = self.qt.model.predict(inp)[0]\n",
    "                target[0][action] = reward + self.config.gamma * t[np.argmax(a)]\n",
    "            self.q.model.fit(inpu, target, epochs=1, verbose=0)\n",
    "#             states.append(inpu)\n",
    "#             targets.append(targets)\n",
    "        print('set target')\n",
    "#         self.q.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "        print('finish replay')\n",
    "        if self.config.epsilon > self.config.epsilon_min:\n",
    "            self.config.epsilon *= self.config.epsilon_decay\n",
    "        \n",
    "\n",
    "    def rescale_color(self, state):\n",
    "        return state / 3.\n",
    "        \n",
    "\n",
    "    def copy_param(self):\n",
    "        self.qt.model.set_weights(self.q.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.q.model.save(path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.q.model = load_model(path)\n",
    "\n",
    "def train():\n",
    "    config = Config()\n",
    "    env = SnakeEnvironment(config.width, config.height)\n",
    "    agent = DQAgent(config)\n",
    "\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    agent.copy_param()\n",
    "\n",
    "    for i in range(EP):\n",
    "        state = env.reset()\n",
    "        \n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        state = np.stack([state for _ in range(2)], axis=-1)\n",
    "        #print(state.shape)\n",
    "        done = False\n",
    "        point = 0\n",
    "        for t in range(5000):\n",
    "            action = agent.act(state)\n",
    "            tmp_state, reward, done = env.act(action)\n",
    "\n",
    "            next_state = np.roll(state, -1, axis=-1)\n",
    "            next_state[:,:,:,-1] = tmp_state\n",
    "\n",
    "            point += reward\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state.copy()\n",
    "            if done:\n",
    "                print('episode {}/{}, score: {}'.format(i, EP, point))\n",
    "\n",
    "                # Print counted frames\n",
    "                print(t)\n",
    "                agent.copy_param()\n",
    "                break\n",
    "            \n",
    "        agent.replay(batch_size)\n",
    "        if i % 100 == 0:\n",
    "            agent.save_model('model/snake.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0/100000, score: -5\n",
      "29\n",
      "do_replay\n",
      "episode 1/100000, score: -5\n",
      "28\n",
      "do_replay\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "env = SnakeEnvironment(config.width, config.height)\n",
    "agent = DQAgent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 26, 16)        816       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 26, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 14, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 10,325\n",
      "Trainable params: 10,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0/100000, score: -5\n",
      "14\n",
      "do_replay\n",
      "episode 1/100000, score: -5\n",
      "22\n",
      "do_replay\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "agent.copy_param()\n",
    "\n",
    "for i in range(EP):\n",
    "    state = env.reset()\n",
    "\n",
    "    state = np.expand_dims(state, axis=0)\n",
    "    state = np.stack([state for _ in range(2)], axis=-1)\n",
    "    #print(state.shape)\n",
    "    done = False\n",
    "    point = 0\n",
    "    for t in range(5000):\n",
    "        action = agent.act(state)\n",
    "        tmp_state, reward, done = env.act(action)\n",
    "\n",
    "        next_state = np.roll(state, -1, axis=-1)\n",
    "        next_state[:,:,:,-1] = tmp_state\n",
    "\n",
    "        point += reward\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state.copy()\n",
    "        if done:\n",
    "            print('episode {}/{}, score: {}'.format(i, EP, point))\n",
    "\n",
    "            # Print counted frames\n",
    "            print(t)\n",
    "            agent.copy_param()\n",
    "            break\n",
    "\n",
    "    agent.replay(batch_size)\n",
    "    if i % 100 == 0:\n",
    "        agent.save_model('model/snake.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chatbot]",
   "language": "python",
   "name": "conda-env-chatbot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
